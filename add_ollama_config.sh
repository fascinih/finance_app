#!/bin/bash

# Script para adicionar configura√ß√£o do Ollama na p√°gina de Configura√ß√µes
echo "ü§ñ Adicionando configura√ß√£o do Ollama..."

# Cores
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'

log() {
    echo -e "${GREEN}[OLLAMA-CONFIG]${NC} $1"
}

info() {
    echo -e "${BLUE}[OLLAMA-CONFIG]${NC} $1"
}

# Verificar se estamos no diret√≥rio correto
if [ ! -f "streamlit_app.py" ]; then
    echo "‚ùå Execute no diret√≥rio da Finance App"
    exit 1
fi

# Fazer backup
cp streamlit_app.py streamlit_app.py.backup_ollama

log "Adicionando fun√ß√µes de configura√ß√£o do Ollama..."

# Usar Python para adicionar as fun√ß√µes
python3 << 'EOF'
import re

# Ler arquivo
with open('streamlit_app.py', 'r') as f:
    content = f.read()

# Adicionar fun√ß√µes helper para Ollama ap√≥s as fun√ß√µes existentes
ollama_functions = '''
def test_ollama_connection(host, model):
    """Testa conex√£o com Ollama."""
    try:
        import requests
        import json
        
        # Testar se Ollama est√° rodando
        response = requests.get(f"{host}/api/version", timeout=5)
        if response.status_code != 200:
            return False, "Ollama n√£o est√° rodando"
        
        # Testar modelo espec√≠fico
        test_data = {
            "model": model,
            "prompt": "Test",
            "stream": False
        }
        
        response = requests.post(f"{host}/api/generate", json=test_data, timeout=10)
        if response.status_code == 200:
            return True, "Conex√£o e modelo funcionando"
        else:
            return False, f"Modelo '{model}' n√£o dispon√≠vel"
            
    except requests.exceptions.ConnectionError:
        return False, "N√£o foi poss√≠vel conectar ao Ollama"
    except requests.exceptions.Timeout:
        return False, "Timeout na conex√£o"
    except Exception as e:
        return False, f"Erro: {str(e)}"

def get_ollama_models(host):
    """Lista modelos dispon√≠veis no Ollama."""
    try:
        import requests
        
        response = requests.get(f"{host}/api/tags", timeout=5)
        if response.status_code == 200:
            data = response.json()
            models = [model['name'] for model in data.get('models', [])]
            return True, models
        else:
            return False, []
            
    except Exception as e:
        return False, []

def save_ollama_config(host, model, temperature, max_tokens):
    """Salva configura√ß√£o do Ollama."""
    config = {
        "ollama_host": host,
        "ollama_model": model,
        "ollama_temperature": temperature,
        "ollama_max_tokens": max_tokens
    }
    
    # Salvar em session_state
    for key, value in config.items():
        st.session_state[key] = value
    
    return True

def load_ollama_config():
    """Carrega configura√ß√£o do Ollama."""
    return {
        "host": st.session_state.get("ollama_host", "http://localhost:11434"),
        "model": st.session_state.get("ollama_model", "deepseek-r1:7b"),
        "temperature": st.session_state.get("ollama_temperature", 0.1),
        "max_tokens": st.session_state.get("ollama_max_tokens", 500)
    }

'''

# Encontrar onde inserir as fun√ß√µes (antes da fun√ß√£o show_settings)
insert_pos = content.find('def show_settings():')
if insert_pos != -1:
    content = content[:insert_pos] + ollama_functions + '\n' + content[insert_pos:]
    print("‚úÖ Fun√ß√µes do Ollama adicionadas")
else:
    print("‚ùå N√£o foi poss√≠vel encontrar fun√ß√£o show_settings")

# Salvar arquivo
with open('streamlit_app.py', 'w') as f:
    f.write(content)

print("‚úÖ Fun√ß√µes helper do Ollama adicionadas")
EOF

log "Modificando fun√ß√£o show_settings para incluir configura√ß√£o do Ollama..."

# Adicionar se√ß√£o do Ollama na fun√ß√£o show_settings
python3 << 'EOF'
# Ler arquivo
with open('streamlit_app.py', 'r') as f:
    content = f.read()

# Encontrar a se√ß√£o tab1 e adicionar configura√ß√£o do Ollama
old_tab1_content = '''    with tab1:
        st.subheader("Configura√ß√µes do Sistema")
        
        api = get_api_client()
        
        # Status dos servi√ßos
        with st.spinner("Verificando status dos servi√ßos..."):
            health = api.get_health()
        
        if health:
            st.success("‚úÖ API conectada com sucesso!")
            
            services = health.get("services", {})
            
            for service_name, service_info in services.items():
                status = service_info.get("status", "unknown") if isinstance(service_info, dict) else "unknown"
                response_time = service_info.get("response_time", 0) if isinstance(service_info, dict) else 0
                
                if status == "healthy":
                    st.success(f"‚úÖ {service_name.title()}: {status} ({response_time:.3f}s)")
                else:
                    st.error(f"‚ùå {service_name.title()}: {status}")
        else:
            st.error("‚ùå N√£o foi poss√≠vel conectar √† API")'''

new_tab1_content = '''    with tab1:
        st.subheader("Configura√ß√µes do Sistema")
        
        # Se√ß√£o API Backend
        st.markdown("#### üîó Backend API")
        api = get_api_client()
        
        # Status dos servi√ßos
        with st.spinner("Verificando status dos servi√ßos..."):
            health = api.get_health()
        
        if health:
            st.success("‚úÖ API conectada com sucesso!")
            
            services = health.get("services", {})
            
            for service_name, service_info in services.items():
                status = service_info.get("status", "unknown") if isinstance(service_info, dict) else "unknown"
                response_time = service_info.get("response_time", 0) if isinstance(service_info, dict) else 0
                
                if status == "healthy":
                    st.success(f"‚úÖ {service_name.title()}: {status} ({response_time:.3f}s)")
                else:
                    st.error(f"‚ùå {service_name.title()}: {status}")
        else:
            st.error("‚ùå N√£o foi poss√≠vel conectar √† API")
        
        st.markdown("---")
        
        # Se√ß√£o Configura√ß√£o do Ollama
        st.markdown("#### ü§ñ Configura√ß√£o do Ollama")
        
        # Carregar configura√ß√£o atual
        config = load_ollama_config()
        
        col1, col2 = st.columns(2)
        
        with col1:
            ollama_host = st.text_input(
                "Host do Ollama",
                value=config["host"],
                help="URL do servidor Ollama (ex: http://localhost:11434)"
            )
            
            ollama_model = st.text_input(
                "Modelo",
                value=config["model"],
                help="Nome do modelo a ser usado (ex: deepseek-r1:7b, mistral:7b)"
            )
        
        with col2:
            ollama_temperature = st.slider(
                "Temperatura",
                min_value=0.0,
                max_value=2.0,
                value=config["temperature"],
                step=0.1,
                help="Controla a criatividade das respostas (0.0 = mais preciso, 2.0 = mais criativo)"
            )
            
            ollama_max_tokens = st.number_input(
                "M√°ximo de Tokens",
                min_value=50,
                max_value=2000,
                value=config["max_tokens"],
                step=50,
                help="N√∫mero m√°ximo de tokens na resposta"
            )
        
        # Bot√µes de a√ß√£o
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            if st.button("üîç Testar Conex√£o"):
                with st.spinner("Testando conex√£o..."):
                    success, message = test_ollama_connection(ollama_host, ollama_model)
                    if success:
                        st.success(f"‚úÖ {message}")
                    else:
                        st.error(f"‚ùå {message}")
        
        with col2:
            if st.button("üìã Listar Modelos"):
                with st.spinner("Buscando modelos..."):
                    success, models = get_ollama_models(ollama_host)
                    if success and models:
                        st.success("Modelos dispon√≠veis:")
                        for model in models:
                            st.write(f"‚Ä¢ {model}")
                    else:
                        st.error("‚ùå N√£o foi poss√≠vel listar modelos")
        
        with col3:
            if st.button("üíæ Salvar Config"):
                save_ollama_config(ollama_host, ollama_model, ollama_temperature, ollama_max_tokens)
                st.success("‚úÖ Configura√ß√£o salva!")
        
        with col4:
            if st.button("üß™ Teste R√°pido"):
                with st.spinner("Testando modelo..."):
                    try:
                        import requests
                        test_data = {
                            "model": ollama_model,
                            "prompt": "Categorize esta transa√ß√£o: PIX Supermercado ABC 150.00",
                            "stream": False
                        }
                        response = requests.post(f"{ollama_host}/api/generate", json=test_data, timeout=15)
                        if response.status_code == 200:
                            result = response.json()
                            st.success("‚úÖ Teste bem-sucedido!")
                            st.write("**Resposta:**", result.get("response", "Sem resposta"))
                        else:
                            st.error("‚ùå Falha no teste")
                    except Exception as e:
                        st.error(f"‚ùå Erro: {str(e)}")
        
        # Informa√ß√µes sobre o Ollama
        with st.expander("‚ÑπÔ∏è Informa√ß√µes sobre Ollama", expanded=False):
            st.markdown("""
            **Ollama** √© um servidor local de LLM que permite executar modelos de IA em sua m√°quina.
            
            **Modelos Recomendados:**
            - `deepseek-r1:7b` - Excelente para an√°lise financeira e racioc√≠nio
            - `mistral:7b` - Modelo vers√°til e r√°pido
            - `llama3.2:3b` - Modelo menor, mais r√°pido
            
            **Como instalar modelos:**
            ```bash
            ollama pull deepseek-r1:7b
            ollama pull mistral:7b
            ```
            
            **Status do Ollama:**
            - ‚úÖ Rodando: Ollama est√° ativo e pronto
            - ‚ùå Parado: Execute `ollama serve` no terminal
            """)'''

# Substituir o conte√∫do
content = content.replace(old_tab1_content, new_tab1_content)

# Salvar arquivo
with open('streamlit_app.py', 'w') as f:
    f.write(content)

print("‚úÖ Se√ß√£o de configura√ß√£o do Ollama adicionada")
EOF

# Verificar se as mudan√ßas foram aplicadas
if grep -q "Configura√ß√£o do Ollama" streamlit_app.py; then
    log "‚úÖ Configura√ß√£o do Ollama adicionada com sucesso"
else
    log "‚ö†Ô∏è Verificar se as mudan√ßas foram aplicadas"
fi

# Verificar sintaxe
python3 -m py_compile streamlit_app.py 2>/dev/null && log "‚úÖ Sintaxe Python v√°lida" || log "‚ö†Ô∏è Verificar sintaxe"

echo ""
echo "=================================================="
echo -e "${GREEN}‚úÖ Configura√ß√£o do Ollama implementada!${NC}"
echo ""
echo "Funcionalidades adicionadas:"
echo "‚Ä¢ üîß Configura√ß√£o de host e modelo"
echo "‚Ä¢ üéõÔ∏è Controles de temperatura e tokens"
echo "‚Ä¢ üîç Teste de conex√£o"
echo "‚Ä¢ üìã Listagem de modelos dispon√≠veis"
echo "‚Ä¢ üíæ Salvamento de configura√ß√µes"
echo "‚Ä¢ üß™ Teste r√°pido de categoriza√ß√£o"
echo "‚Ä¢ ‚ÑπÔ∏è Informa√ß√µes e documenta√ß√£o"
echo ""
echo "Agora reinicie o Streamlit:"
echo "‚Ä¢ Ctrl+C para parar"
echo "‚Ä¢ ./start_simple.sh para reiniciar"
echo ""
echo "V√° para Configura√ß√µes ‚Üí Sistema para configurar o Ollama!"
echo "=================================================="

