#!/bin/bash

# Script para corrigir status do Ollama em todas as p√°ginas
echo "üîß Corrigindo status do Ollama em todas as p√°ginas..."

# Cores
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'

log() {
    echo -e "${GREEN}[OLLAMA-STATUS-FIX]${NC} $1"
}

info() {
    echo -e "${BLUE}[OLLAMA-STATUS-FIX]${NC} $1"
}

# Verificar se estamos no diret√≥rio correto
if [ ! -f "streamlit_app.py" ]; then
    echo "‚ùå Execute no diret√≥rio da Finance App"
    exit 1
fi

# Fazer backup
cp streamlit_app.py streamlit_app.py.backup_status

log "Adicionando fun√ß√£o centralizada de status do Ollama..."

# Usar Python para adicionar fun√ß√£o centralizada
python3 << 'EOF'
import re

# Ler arquivo
with open('streamlit_app.py', 'r') as f:
    content = f.read()

# Adicionar fun√ß√£o centralizada de status do Ollama
ollama_status_function = '''
def get_ollama_status():
    """Fun√ß√£o centralizada para verificar status do Ollama."""
    try:
        # Carregar configura√ß√£o salva
        config = load_ollama_config()
        host = config["host"]
        model = config["model"]
        
        # Verificar se Ollama est√° rodando
        import requests
        
        # Teste r√°pido de conex√£o
        response = requests.get(f"{host}/api/version", timeout=3)
        if response.status_code != 200:
            return {
                "status": "offline",
                "message": "Ollama n√£o est√° rodando",
                "host": host,
                "model": model
            }
        
        # Verificar se modelo est√° dispon√≠vel
        response = requests.get(f"{host}/api/tags", timeout=5)
        if response.status_code == 200:
            data = response.json()
            available_models = [m['name'] for m in data.get('models', [])]
            
            if model in available_models:
                return {
                    "status": "online",
                    "message": f"Ollama funcionando com {model}",
                    "host": host,
                    "model": model,
                    "available_models": available_models
                }
            else:
                return {
                    "status": "model_missing",
                    "message": f"Modelo {model} n√£o encontrado",
                    "host": host,
                    "model": model,
                    "available_models": available_models
                }
        else:
            return {
                "status": "error",
                "message": "Erro ao verificar modelos",
                "host": host,
                "model": model
            }
            
    except requests.exceptions.ConnectionError:
        return {
            "status": "offline",
            "message": "Ollama n√£o est√° rodando",
            "host": config.get("host", "localhost:11434"),
            "model": config.get("model", "deepseek-r1:7b")
        }
    except Exception as e:
        return {
            "status": "error",
            "message": f"Erro: {str(e)}",
            "host": config.get("host", "localhost:11434"),
            "model": config.get("model", "deepseek-r1:7b")
        }

'''

# Encontrar onde inserir a fun√ß√£o (antes da fun√ß√£o show_settings)
insert_pos = content.find('def show_settings():')
if insert_pos != -1:
    content = content[:insert_pos] + ollama_status_function + '\n' + content[insert_pos:]
    print("‚úÖ Fun√ß√£o centralizada de status adicionada")
else:
    print("‚ùå N√£o foi poss√≠vel encontrar fun√ß√£o show_settings")

# Salvar arquivo
with open('streamlit_app.py', 'w') as f:
    f.write(content)

print("‚úÖ Fun√ß√£o centralizada criada")
EOF

log "Atualizando Dashboard para usar status centralizado..."

# Atualizar Dashboard
python3 << 'EOF'
# Ler arquivo
with open('streamlit_app.py', 'r') as f:
    content = f.read()

# Encontrar e substituir a se√ß√£o de status do Ollama no Dashboard
old_ollama_status = '''        with col3:
            ollama_status = "unknown"
            status_color = "üî¥"
            st.write(f"{status_color} **Ollama:** {ollama_status}")'''

new_ollama_status = '''        with col3:
            ollama_info = get_ollama_status()
            if ollama_info["status"] == "online":
                status_color = "üü¢"
                ollama_status = "funcionando"
            elif ollama_info["status"] == "offline":
                status_color = "üî¥"
                ollama_status = "offline"
            elif ollama_info["status"] == "model_missing":
                status_color = "üü°"
                ollama_status = "modelo n√£o encontrado"
            else:
                status_color = "üî¥"
                ollama_status = "erro"
            st.write(f"{status_color} **Ollama:** {ollama_status}")'''

content = content.replace(old_ollama_status, new_ollama_status)

# Salvar arquivo
with open('streamlit_app.py', 'w') as f:
    f.write(content)

print("‚úÖ Dashboard atualizado")
EOF

log "Criando p√°gina LLM melhorada..."

# Criar nova p√°gina LLM
python3 << 'EOF'
# Ler arquivo
with open('streamlit_app.py', 'r') as f:
    content = f.read()

# Encontrar e substituir a p√°gina LLM completa
# Primeiro, encontrar onde come√ßa a p√°gina LLM
llm_start = content.find('elif page == "ü§ñ LLM":')
if llm_start != -1:
    # Encontrar onde termina (pr√≥ximo elif ou final da fun√ß√£o)
    llm_end = content.find('elif page == "üè¶ APIs":', llm_start)
    if llm_end == -1:
        llm_end = content.find('def main():', llm_start)
    
    if llm_end != -1:
        # Substituir toda a se√ß√£o LLM
        new_llm_page = '''elif page == "ü§ñ LLM":
        st.header("ü§ñ LLM & Intelig√™ncia Artificial")
        st.markdown("Configure e teste as funcionalidades de IA para an√°lise financeira inteligente.")
        
        tab1, tab2, tab3, tab4 = st.tabs(["üîß Status", "üè∑Ô∏è Categoriza√ß√£o", "üí° Insights", "üîÑ Recorrentes"])
        
        with tab1:
            st.subheader("Status do Ollama")
            
            # Verificar status do Ollama
            ollama_info = get_ollama_status()
            
            if ollama_info["status"] == "online":
                st.success(f"‚úÖ {ollama_info['message']}")
                
                col1, col2 = st.columns(2)
                with col1:
                    st.info(f"**Host:** {ollama_info['host']}")
                    st.info(f"**Modelo Ativo:** {ollama_info['model']}")
                
                with col2:
                    if "available_models" in ollama_info:
                        st.info(f"**Modelos Dispon√≠veis:** {len(ollama_info['available_models'])}")
                        with st.expander("Ver todos os modelos"):
                            for model in ollama_info['available_models']:
                                st.write(f"‚Ä¢ {model}")
                
                # Teste r√°pido de categoriza√ß√£o
                st.markdown("#### üß™ Teste de Categoriza√ß√£o")
                test_transaction = st.text_input(
                    "Digite uma transa√ß√£o para testar:",
                    value="PIX Supermercado ABC 150.00",
                    help="Exemplo: PIX Supermercado ABC 150.00"
                )
                
                if st.button("üîç Categorizar"):
                    with st.spinner("Categorizando..."):
                        try:
                            import requests
                            test_data = {
                                "model": ollama_info["model"],
                                "prompt": f"Categorize esta transa√ß√£o financeira em uma das categorias: Alimenta√ß√£o, Transporte, Moradia, Sa√∫de, Educa√ß√£o, Lazer, Renda, Outros. Responda apenas a categoria: {test_transaction}",
                                "stream": False,
                                "options": {"num_predict": 10, "temperature": 0.1}
                            }
                            response = requests.post(f"{ollama_info['host']}/api/generate", json=test_data, timeout=20)
                            if response.status_code == 200:
                                result = response.json()
                                category = result.get("response", "").strip()
                                st.success(f"‚úÖ **Categoria sugerida:** {category}")
                            else:
                                st.error("‚ùå Erro na categoriza√ß√£o")
                        except Exception as e:
                            st.error(f"‚ùå Erro: {str(e)}")
                            
            elif ollama_info["status"] == "offline":
                st.error(f"‚ùå {ollama_info['message']}")
                st.info("üí° **Como resolver:**")
                st.code("ollama serve")
                st.markdown("Execute o comando acima no terminal para iniciar o Ollama.")
                
            elif ollama_info["status"] == "model_missing":
                st.warning(f"‚ö†Ô∏è {ollama_info['message']}")
                st.info("üí° **Como resolver:**")
                st.code(f"ollama pull {ollama_info['model']}")
                st.markdown("Execute o comando acima para baixar o modelo.")
                
                if "available_models" in ollama_info and ollama_info["available_models"]:
                    st.info("**Modelos dispon√≠veis:**")
                    for model in ollama_info["available_models"]:
                        st.write(f"‚Ä¢ {model}")
                        
            else:
                st.error(f"‚ùå {ollama_info['message']}")
            
            # Link para configura√ß√µes
            st.markdown("---")
            st.info("üîß **Configurar Ollama:** V√° para Configura√ß√µes ‚Üí Sistema ‚Üí Configura√ß√£o do Ollama")
        
        with tab2:
            st.subheader("Categoriza√ß√£o Autom√°tica")
            st.info("üöß Em desenvolvimento: Interface para configurar regras de categoriza√ß√£o autom√°tica")
            
        with tab3:
            st.subheader("Insights Inteligentes")
            st.info("üöß Em desenvolvimento: An√°lise inteligente de padr√µes de gastos")
            
        with tab4:
            st.subheader("Detec√ß√£o de Recorrentes")
            st.info("üöß Em desenvolvimento: Identifica√ß√£o autom√°tica de gastos recorrentes")
        
        '''
        
        # Substituir o conte√∫do
        content = content[:llm_start] + new_llm_page + content[llm_end:]
        
        print("‚úÖ P√°gina LLM atualizada")
    else:
        print("‚ùå N√£o foi poss√≠vel encontrar final da p√°gina LLM")
else:
    print("‚ùå N√£o foi poss√≠vel encontrar p√°gina LLM")

# Salvar arquivo
with open('streamlit_app.py', 'w') as f:
    f.write(content)

print("‚úÖ P√°gina LLM melhorada")
EOF

# Verificar sintaxe
python3 -m py_compile streamlit_app.py 2>/dev/null && log "‚úÖ Sintaxe Python v√°lida" || log "‚ö†Ô∏è Verificar sintaxe"

echo ""
echo "=================================================="
echo -e "${GREEN}‚úÖ Status do Ollama corrigido em todas as p√°ginas!${NC}"
echo ""
echo "Melhorias implementadas:"
echo "‚Ä¢ üéØ Fun√ß√£o centralizada de status"
echo "‚Ä¢ üìä Dashboard com status real do Ollama"
echo "‚Ä¢ ü§ñ P√°gina LLM completamente renovada"
echo "‚Ä¢ üß™ Teste de categoriza√ß√£o integrado"
echo "‚Ä¢ üí° Dicas de resolu√ß√£o de problemas"
echo "‚Ä¢ üîó Links para configura√ß√£o"
echo ""
echo "Agora reinicie o Streamlit:"
echo "‚Ä¢ Ctrl+C para parar"
echo "‚Ä¢ ./start_simple.sh para reiniciar"
echo ""
echo "O status do Ollama deve aparecer corretamente!"
echo "=================================================="

