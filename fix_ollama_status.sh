#!/bin/bash

# Script para corrigir status do Ollama em todas as pÃ¡ginas
echo "ğŸ”§ Corrigindo status do Ollama em todas as pÃ¡ginas..."

# Cores
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'

log() {
    echo -e "${GREEN}[OLLAMA-STATUS-FIX]${NC} $1"
}

info() {
    echo -e "${BLUE}[OLLAMA-STATUS-FIX]${NC} $1"
}

# Verificar se estamos no diretÃ³rio correto
if [ ! -f "streamlit_app.py" ]; then
    echo "âŒ Execute no diretÃ³rio da Finance App"
    exit 1
fi

# Fazer backup
cp streamlit_app.py streamlit_app.py.backup_status

log "Adicionando funÃ§Ã£o centralizada de status do Ollama..."

# Usar Python para adicionar funÃ§Ã£o centralizada
python3 << 'EOF'
import re

# Ler arquivo
with open('streamlit_app.py', 'r') as f:
    content = f.read()

# Adicionar funÃ§Ã£o centralizada de status do Ollama
ollama_status_function = '''
def get_ollama_status():
    """FunÃ§Ã£o centralizada para verificar status do Ollama."""
    try:
        # Carregar configuraÃ§Ã£o salva
        config = load_ollama_config()
        host = config["host"]
        model = config["model"]
        
        # Verificar se Ollama estÃ¡ rodando
        import requests
        
        # Teste rÃ¡pido de conexÃ£o
        response = requests.get(f"{host}/api/version", timeout=3)
        if response.status_code != 200:
            return {
                "status": "offline",
                "message": "Ollama nÃ£o estÃ¡ rodando",
                "host": host,
                "model": model
            }
        
        # Verificar se modelo estÃ¡ disponÃ­vel
        response = requests.get(f"{host}/api/tags", timeout=5)
        if response.status_code == 200:
            data = response.json()
            available_models = [m['name'] for m in data.get('models', [])]
            
            if model in available_models:
                return {
                    "status": "online",
                    "message": f"Ollama funcionando com {model}",
                    "host": host,
                    "model": model,
                    "available_models": available_models
                }
            else:
                return {
                    "status": "model_missing",
                    "message": f"Modelo {model} nÃ£o encontrado",
                    "host": host,
                    "model": model,
                    "available_models": available_models
                }
        else:
            return {
                "status": "error",
                "message": "Erro ao verificar modelos",
                "host": host,
                "model": model
            }
            
    except requests.exceptions.ConnectionError:
        return {
            "status": "offline",
            "message": "Ollama nÃ£o estÃ¡ rodando",
            "host": config.get("host", "localhost:11434"),
            "model": config.get("model", "deepseek-r1:7b")
        }
    except Exception as e:
        return {
            "status": "error",
            "message": f"Erro: {str(e)}",
            "host": config.get("host", "localhost:11434"),
            "model": config.get("model", "deepseek-r1:7b")
        }

'''

# Encontrar onde inserir a funÃ§Ã£o (antes da funÃ§Ã£o show_settings)
insert_pos = content.find('def show_settings():')
if insert_pos != -1:
    content = content[:insert_pos] + ollama_status_function + '\n' + content[insert_pos:]
    print("âœ… FunÃ§Ã£o centralizada de status adicionada")
else:
    print("âŒ NÃ£o foi possÃ­vel encontrar funÃ§Ã£o show_settings")

# Salvar arquivo
with open('streamlit_app.py', 'w') as f:
    f.write(content)

print("âœ… FunÃ§Ã£o centralizada criada")
EOF

log "Atualizando Dashboard para usar status centralizado..."

# Atualizar Dashboard
python3 << 'EOF'
# Ler arquivo
with open('streamlit_app.py', 'r') as f:
    content = f.read()

# Encontrar e substituir a seÃ§Ã£o de status do Ollama no Dashboard
old_ollama_status = '''        with col3:
            ollama_status = "unknown"
            status_color = "ğŸ”´"
            st.write(f"{status_color} **Ollama:** {ollama_status}")'''

new_ollama_status = '''        with col3:
            ollama_info = get_ollama_status()
            if ollama_info["status"] == "online":
                status_color = "ğŸŸ¢"
                ollama_status = "funcionando"
            elif ollama_info["status"] == "offline":
                status_color = "ğŸ”´"
                ollama_status = "offline"
            elif ollama_info["status"] == "model_missing":
                status_color = "ğŸŸ¡"
                ollama_status = "modelo nÃ£o encontrado"
            else:
                status_color = "ğŸ”´"
                ollama_status = "erro"
            st.write(f"{status_color} **Ollama:** {ollama_status}")'''

content = content.replace(old_ollama_status, new_ollama_status)

# Salvar arquivo
with open('streamlit_app.py', 'w') as f:
    f.write(content)

print("âœ… Dashboard atualizado")
EOF

log "Criando pÃ¡gina LLM melhorada..."

# Criar nova pÃ¡gina LLM
python3 << 'EOF'
# Ler arquivo
with open('streamlit_app.py', 'r') as f:
    content = f.read()

# Encontrar e substituir a pÃ¡gina LLM completa
# Primeiro, encontrar onde comeÃ§a a pÃ¡gina LLM
llm_start = content.find('elif page == "ğŸ¤– LLM":')
if llm_start != -1:
    # Encontrar onde termina (prÃ³ximo elif ou final da funÃ§Ã£o)
    llm_end = content.find('elif page == "ğŸ¦ APIs":', llm_start)
    if llm_end == -1:
        llm_end = content.find('def main():', llm_start)
    
    if llm_end != -1:
        # Substituir toda a seÃ§Ã£o LLM
        new_llm_page = '''elif page == "ğŸ¤– LLM":
        st.header("ğŸ¤– LLM & InteligÃªncia Artificial")
        st.markdown("Configure e teste as funcionalidades de IA para anÃ¡lise financeira inteligente.")
        
        tab1, tab2, tab3, tab4 = st.tabs(["ğŸ”§ Status", "ğŸ·ï¸ CategorizaÃ§Ã£o", "ğŸ’¡ Insights", "ğŸ”„ Recorrentes"])
        
        with tab1:
            st.subheader("Status do Ollama")
            
            # Verificar status do Ollama
            ollama_info = get_ollama_status()
            
            if ollama_info["status"] == "online":
                st.success(f"âœ… {ollama_info['message']}")
                
                col1, col2 = st.columns(2)
                with col1:
                    st.info(f"**Host:** {ollama_info['host']}")
                    st.info(f"**Modelo Ativo:** {ollama_info['model']}")
                
                with col2:
                    if "available_models" in ollama_info:
                        st.info(f"**Modelos DisponÃ­veis:** {len(ollama_info['available_models'])}")
                        with st.expander("Ver todos os modelos"):
                            for model in ollama_info['available_models']:
                                st.write(f"â€¢ {model}")
                
                # Teste rÃ¡pido de categorizaÃ§Ã£o
                st.markdown("#### ğŸ§ª Teste de CategorizaÃ§Ã£o")
                test_transaction = st.text_input(
                    "Digite uma transaÃ§Ã£o para testar:",
                    value="PIX Supermercado ABC 150.00",
                    help="Exemplo: PIX Supermercado ABC 150.00"
                )
                
                if st.button("ğŸ” Categorizar"):
                    with st.spinner("Categorizando..."):
                        try:
                            import requests
                            test_data = {
                                "model": ollama_info["model"],
                                "prompt": f"Categorize esta transaÃ§Ã£o financeira em uma das categorias: AlimentaÃ§Ã£o, Transporte, Moradia, SaÃºde, EducaÃ§Ã£o, Lazer, Renda, Outros. Responda apenas a categoria: {test_transaction}",
                                "stream": False,
                                "options": {"num_predict": 10, "temperature": 0.1}
                            }
                            response = requests.post(f"{ollama_info['host']}/api/generate", json=test_data, timeout=20)
                            if response.status_code == 200:
                                result = response.json()
                                category = result.get("response", "").strip()
                                st.success(f"âœ… **Categoria sugerida:** {category}")
                            else:
                                st.error("âŒ Erro na categorizaÃ§Ã£o")
                        except Exception as e:
                            st.error(f"âŒ Erro: {str(e)}")
                            
            elif ollama_info["status"] == "offline":
                st.error(f"âŒ {ollama_info['message']}")
                st.info("ğŸ’¡ **Como resolver:**")
                st.code("ollama serve")
                st.markdown("Execute o comando acima no terminal para iniciar o Ollama.")
                
            elif ollama_info["status"] == "model_missing":
                st.warning(f"âš ï¸ {ollama_info['message']}")
                st.info("ğŸ’¡ **Como resolver:**")
                st.code(f"ollama pull {ollama_info['model']}")
                st.markdown("Execute o comando acima para baixar o modelo.")
                
                if "available_models" in ollama_info and ollama_info["available_models"]:
                    st.info("**Modelos disponÃ­veis:**")
                    for model in ollama_info["available_models"]:
                        st.write(f"â€¢ {model}")
                        
            else:
                st.error(f"âŒ {ollama_info['message']}")
            
            # Link para configuraÃ§Ãµes
            st.markdown("---")
            st.info("ğŸ”§ **Configurar Ollama:** VÃ¡ para ConfiguraÃ§Ãµes â†’ Sistema â†’ ConfiguraÃ§Ã£o do Ollama")
        
        with tab2:
            st.subheader("CategorizaÃ§Ã£o AutomÃ¡tica")
            st.info("ğŸš§ Em desenvolvimento: Interface para configurar regras de categorizaÃ§Ã£o automÃ¡tica")
            
        with tab3:
            st.subheader("Insights Inteligentes")
            st.info("ğŸš§ Em desenvolvimento: AnÃ¡lise inteligente de padrÃµes de gastos")
            
        with tab4:
            st.subheader("DetecÃ§Ã£o de Recorrentes")
            st.info("ğŸš§ Em desenvolvimento: IdentificaÃ§Ã£o automÃ¡tica de gastos recorrentes")
        
        '''
        
        # Substituir o conteÃºdo
        content = content[:llm_start] + new_llm_page + content[llm_end:]
        
        print("âœ… PÃ¡gina LLM atualizada")
    else:
        print("âŒ NÃ£o foi possÃ­vel encontrar final da pÃ¡gina LLM")
else:
    print("âŒ NÃ£o foi possÃ­vel encontrar pÃ¡gina LLM")

# Salvar arquivo
with open('streamlit_app.py', 'w') as f:
    f.write(content)

print("âœ… PÃ¡gina LLM melhorada")
EOF

# Verificar sintaxe
python3 -m py_compile streamlit_app.py 2>/dev/null && log "âœ… Sintaxe Python vÃ¡lida" || log "âš ï¸ Verificar sintaxe"

echo ""
echo "=================================================="
echo -e "${GREEN}âœ… Status do Ollama corrigido em todas as pÃ¡ginas!${NC}"
echo ""
echo "Melhorias implementadas:"
echo "â€¢ ğŸ¯ FunÃ§Ã£o centralizada de status"
echo "â€¢ ğŸ“Š Dashboard com status real do Ollama"
echo "â€¢ ğŸ¤– PÃ¡gina LLM completamente renovada"
echo "â€¢ ğŸ§ª Teste de categorizaÃ§Ã£o integrado"
echo "â€¢ ğŸ’¡ Dicas de resoluÃ§Ã£o de problemas"
echo "â€¢ ğŸ”— Links para configuraÃ§Ã£o"
echo ""
echo "Agora reinicie o Streamlit:"
echo "â€¢ Ctrl+C para parar"
echo "â€¢ ./start_simple.sh para reiniciar"
echo ""
echo "O status do Ollama deve aparecer corretamente!"
echo "=================================================="

