#!/bin/bash

# Script para adicionar pÃ¡gina LLM e corrigir status do Ollama
echo "ğŸ¤– Adicionando pÃ¡gina LLM e corrigindo status..."

# Cores
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'

log() {
    echo -e "${GREEN}[LLM-PAGE-ADD]${NC} $1"
}

info() {
    echo -e "${BLUE}[LLM-PAGE-ADD]${NC} $1"
}

# Verificar se estamos no diretÃ³rio correto
if [ ! -f "streamlit_app.py" ]; then
    echo "âŒ Execute no diretÃ³rio da Finance App"
    exit 1
fi

# Fazer backup
cp streamlit_app.py streamlit_app.py.backup_llm_page

log "Adicionando pÃ¡gina LLM ao menu de navegaÃ§Ã£o..."

# Usar Python para modificar o arquivo
python3 << 'EOF'
# Ler arquivo
with open('streamlit_app.py', 'r') as f:
    content = f.read()

# 1. Adicionar pÃ¡gina LLM ao menu de navegaÃ§Ã£o
old_menu = '''        page = st.selectbox(
            "NavegaÃ§Ã£o",
            ["ğŸ  Dashboard", "ğŸ’³ TransaÃ§Ãµes", "ğŸ“Š AnÃ¡lises", "âš™ï¸ ConfiguraÃ§Ãµes"]
        )'''

new_menu = '''        page = st.selectbox(
            "NavegaÃ§Ã£o",
            ["ğŸ  Dashboard", "ğŸ’³ TransaÃ§Ãµes", "ğŸ“Š AnÃ¡lises", "ğŸ¤– LLM", "âš™ï¸ ConfiguraÃ§Ãµes"]
        )'''

content = content.replace(old_menu, new_menu)

# 2. Corrigir status do Ollama no Dashboard (se ainda nÃ£o foi corrigido)
if "get_ollama_status()" not in content:
    # Adicionar funÃ§Ã£o get_ollama_status se nÃ£o existir
    if "def get_ollama_status():" not in content:
        ollama_status_function = '''
def get_ollama_status():
    """FunÃ§Ã£o centralizada para verificar status do Ollama."""
    try:
        # Carregar configuraÃ§Ã£o salva
        config = load_ollama_config()
        host = config["host"]
        model = config["model"]
        
        # Verificar se Ollama estÃ¡ rodando
        import requests
        
        # Teste rÃ¡pido de conexÃ£o
        response = requests.get(f"{host}/api/version", timeout=3)
        if response.status_code != 200:
            return {
                "status": "offline",
                "message": "Ollama nÃ£o estÃ¡ rodando",
                "host": host,
                "model": model
            }
        
        # Verificar se modelo estÃ¡ disponÃ­vel
        response = requests.get(f"{host}/api/tags", timeout=5)
        if response.status_code == 200:
            data = response.json()
            available_models = [m['name'] for m in data.get('models', [])]
            
            if model in available_models:
                return {
                    "status": "online",
                    "message": f"Ollama funcionando com {model}",
                    "host": host,
                    "model": model,
                    "available_models": available_models
                }
            else:
                return {
                    "status": "model_missing",
                    "message": f"Modelo {model} nÃ£o encontrado",
                    "host": host,
                    "model": model,
                    "available_models": available_models
                }
        else:
            return {
                "status": "error",
                "message": "Erro ao verificar modelos",
                "host": host,
                "model": model
            }
            
    except requests.exceptions.ConnectionError:
        return {
            "status": "offline",
            "message": "Ollama nÃ£o estÃ¡ rodando",
            "host": config.get("host", "localhost:11434"),
            "model": config.get("model", "deepseek-r1:7b")
        }
    except Exception as e:
        return {
            "status": "error",
            "message": f"Erro: {str(e)}",
            "host": config.get("host", "localhost:11434"),
            "model": config.get("model", "deepseek-r1:7b")
        }

'''
        
        # Inserir funÃ§Ã£o antes de show_dashboard
        insert_pos = content.find('def show_dashboard():')
        if insert_pos != -1:
            content = content[:insert_pos] + ollama_status_function + '\n' + content[insert_pos:]

# 3. Adicionar funÃ§Ã£o show_llm
show_llm_function = '''
def show_llm():
    """Exibe pÃ¡gina de LLM e IA."""
    st.header("ğŸ¤– LLM & InteligÃªncia Artificial")
    st.markdown("Configure e teste as funcionalidades de IA para anÃ¡lise financeira inteligente.")
    
    tab1, tab2, tab3, tab4 = st.tabs(["ğŸ”§ Status", "ğŸ·ï¸ CategorizaÃ§Ã£o", "ğŸ’¡ Insights", "ğŸ”„ Recorrentes"])
    
    with tab1:
        st.subheader("Status do Ollama")
        
        # Verificar status do Ollama
        ollama_info = get_ollama_status()
        
        if ollama_info["status"] == "online":
            st.success(f"âœ… {ollama_info['message']}")
            
            col1, col2 = st.columns(2)
            with col1:
                st.info(f"**Host:** {ollama_info['host']}")
                st.info(f"**Modelo Ativo:** {ollama_info['model']}")
            
            with col2:
                if "available_models" in ollama_info:
                    st.info(f"**Modelos DisponÃ­veis:** {len(ollama_info['available_models'])}")
                    with st.expander("Ver todos os modelos"):
                        for model in ollama_info['available_models']:
                            st.write(f"â€¢ {model}")
            
            # Teste rÃ¡pido de categorizaÃ§Ã£o
            st.markdown("#### ğŸ§ª Teste de CategorizaÃ§Ã£o")
            test_transaction = st.text_input(
                "Digite uma transaÃ§Ã£o para testar:",
                value="PIX Supermercado ABC 150.00",
                help="Exemplo: PIX Supermercado ABC 150.00"
            )
            
            if st.button("ğŸ” Categorizar"):
                with st.spinner("Categorizando..."):
                    try:
                        import requests
                        test_data = {
                            "model": ollama_info["model"],
                            "prompt": f"Categorize esta transaÃ§Ã£o financeira em uma das categorias: AlimentaÃ§Ã£o, Transporte, Moradia, SaÃºde, EducaÃ§Ã£o, Lazer, Renda, Outros. Responda apenas a categoria: {test_transaction}",
                            "stream": False,
                            "options": {"num_predict": 10, "temperature": 0.1}
                        }
                        response = requests.post(f"{ollama_info['host']}/api/generate", json=test_data, timeout=20)
                        if response.status_code == 200:
                            result = response.json()
                            category = result.get("response", "").strip()
                            st.success(f"âœ… **Categoria sugerida:** {category}")
                        else:
                            st.error("âŒ Erro na categorizaÃ§Ã£o")
                    except Exception as e:
                        st.error(f"âŒ Erro: {str(e)}")
                        
        elif ollama_info["status"] == "offline":
            st.error(f"âŒ {ollama_info['message']}")
            st.info("ğŸ’¡ **Como resolver:**")
            st.code("ollama serve")
            st.markdown("Execute o comando acima no terminal para iniciar o Ollama.")
            
        elif ollama_info["status"] == "model_missing":
            st.warning(f"âš ï¸ {ollama_info['message']}")
            st.info("ğŸ’¡ **Como resolver:**")
            st.code(f"ollama pull {ollama_info['model']}")
            st.markdown("Execute o comando acima para baixar o modelo.")
            
            if "available_models" in ollama_info and ollama_info["available_models"]:
                st.info("**Modelos disponÃ­veis:**")
                for model in ollama_info["available_models"]:
                    st.write(f"â€¢ {model}")
                    
        else:
            st.error(f"âŒ {ollama_info['message']}")
        
        # Link para configuraÃ§Ãµes
        st.markdown("---")
        st.info("ğŸ”§ **Configurar Ollama:** VÃ¡ para ConfiguraÃ§Ãµes â†’ Sistema â†’ ConfiguraÃ§Ã£o do Ollama")
    
    with tab2:
        st.subheader("CategorizaÃ§Ã£o AutomÃ¡tica")
        st.info("ğŸš§ Em desenvolvimento: Interface para configurar regras de categorizaÃ§Ã£o automÃ¡tica")
        
    with tab3:
        st.subheader("Insights Inteligentes")
        st.info("ğŸš§ Em desenvolvimento: AnÃ¡lise inteligente de padrÃµes de gastos")
        
    with tab4:
        st.subheader("DetecÃ§Ã£o de Recorrentes")
        st.info("ğŸš§ Em desenvolvimento: IdentificaÃ§Ã£o automÃ¡tica de gastos recorrentes")

'''

# Inserir funÃ§Ã£o show_llm antes da funÃ§Ã£o main
insert_pos = content.find('def main():')
if insert_pos != -1:
    content = content[:insert_pos] + show_llm_function + '\n' + content[insert_pos:]

# 4. Adicionar roteamento para pÃ¡gina LLM
old_routing = '''    elif page == "âš™ï¸ ConfiguraÃ§Ãµes":
        show_settings()'''

new_routing = '''    elif page == "ğŸ¤– LLM":
        show_llm()
    elif page == "âš™ï¸ ConfiguraÃ§Ãµes":
        show_settings()'''

content = content.replace(old_routing, new_routing)

# 5. Corrigir status do Ollama no Dashboard
if "ollama_status = \"unknown\"" in content:
    old_ollama_dashboard = '''            ollama_status = "unknown"
            status_color = "ğŸ”´"
            st.write(f"{status_color} **Ollama:** {ollama_status}")'''
    
    new_ollama_dashboard = '''            ollama_info = get_ollama_status()
            if ollama_info["status"] == "online":
                status_color = "ğŸŸ¢"
                ollama_status = "funcionando"
            elif ollama_info["status"] == "offline":
                status_color = "ğŸ”´"
                ollama_status = "offline"
            elif ollama_info["status"] == "model_missing":
                status_color = "ğŸŸ¡"
                ollama_status = "modelo nÃ£o encontrado"
            else:
                status_color = "ğŸ”´"
                ollama_status = "erro"
            st.write(f"{status_color} **Ollama:** {ollama_status}")'''
    
    content = content.replace(old_ollama_dashboard, new_ollama_dashboard)

# Salvar arquivo
with open('streamlit_app.py', 'w') as f:
    f.write(content)

print("âœ… PÃ¡gina LLM adicionada e status corrigido")
EOF

# Verificar se as mudanÃ§as foram aplicadas
if grep -q "ğŸ¤– LLM" streamlit_app.py; then
    log "âœ… PÃ¡gina LLM adicionada ao menu"
else
    log "âš ï¸ Verificar se pÃ¡gina LLM foi adicionada"
fi

if grep -q "def show_llm" streamlit_app.py; then
    log "âœ… FunÃ§Ã£o show_llm criada"
else
    log "âš ï¸ Verificar se funÃ§Ã£o show_llm foi criada"
fi

# Verificar sintaxe
python3 -m py_compile streamlit_app.py 2>/dev/null && log "âœ… Sintaxe Python vÃ¡lida" || log "âš ï¸ Verificar sintaxe"

echo ""
echo "=================================================="
echo -e "${GREEN}âœ… PÃ¡gina LLM adicionada com sucesso!${NC}"
echo ""
echo "Funcionalidades implementadas:"
echo "â€¢ ğŸ¤– Nova pÃ¡gina LLM no menu de navegaÃ§Ã£o"
echo "â€¢ ğŸ”§ Status detalhado do Ollama"
echo "â€¢ ğŸ§ª Teste de categorizaÃ§Ã£o integrado"
echo "â€¢ ğŸ“Š Dashboard com status real do Ollama"
echo "â€¢ ğŸ’¡ Dicas de resoluÃ§Ã£o de problemas"
echo "â€¢ ğŸ”— Links para configuraÃ§Ã£o"
echo ""
echo "Agora reinicie o Streamlit:"
echo "â€¢ Ctrl+C para parar"
echo "â€¢ ./start_simple.sh para reiniciar"
echo ""
echo "VocÃª verÃ¡ a nova pÃ¡gina ğŸ¤– LLM no menu!"
echo "=================================================="

