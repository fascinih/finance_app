#!/bin/bash

# Script para adicionar p√°gina LLM e corrigir status do Ollama
echo "ü§ñ Adicionando p√°gina LLM e corrigindo status..."

# Cores
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'

log() {
    echo -e "${GREEN}[LLM-PAGE-ADD]${NC} $1"
}

info() {
    echo -e "${BLUE}[LLM-PAGE-ADD]${NC} $1"
}

# Verificar se estamos no diret√≥rio correto
if [ ! -f "streamlit_app.py" ]; then
    echo "‚ùå Execute no diret√≥rio da Finance App"
    exit 1
fi

# Fazer backup
cp streamlit_app.py streamlit_app.py.backup_llm_page

log "Adicionando p√°gina LLM ao menu de navega√ß√£o..."

# Usar Python para modificar o arquivo
python3 << 'EOF'
# Ler arquivo
with open('streamlit_app.py', 'r') as f:
    content = f.read()

# 1. Adicionar p√°gina LLM ao menu de navega√ß√£o
old_menu = '''        page = st.selectbox(
            "Navega√ß√£o",
            ["üè† Dashboard", "üí≥ Transa√ß√µes", "üìä An√°lises", "‚öôÔ∏è Configura√ß√µes"]
        )'''

new_menu = '''        page = st.selectbox(
            "Navega√ß√£o",
            ["üè† Dashboard", "üí≥ Transa√ß√µes", "üìä An√°lises", "ü§ñ LLM", "‚öôÔ∏è Configura√ß√µes"]
        )'''

content = content.replace(old_menu, new_menu)

# 2. Corrigir status do Ollama no Dashboard (se ainda n√£o foi corrigido)
if "get_ollama_status()" not in content:
    # Adicionar fun√ß√£o get_ollama_status se n√£o existir
    if "def get_ollama_status():" not in content:
        ollama_status_function = '''
def get_ollama_status():
    """Fun√ß√£o centralizada para verificar status do Ollama."""
    try:
        # Carregar configura√ß√£o salva
        config = load_ollama_config()
        host = config["host"]
        model = config["model"]
        
        # Verificar se Ollama est√° rodando
        import requests
        
        # Teste r√°pido de conex√£o
        response = requests.get(f"{host}/api/version", timeout=3)
        if response.status_code != 200:
            return {
                "status": "offline",
                "message": "Ollama n√£o est√° rodando",
                "host": host,
                "model": model
            }
        
        # Verificar se modelo est√° dispon√≠vel
        response = requests.get(f"{host}/api/tags", timeout=5)
        if response.status_code == 200:
            data = response.json()
            available_models = [m['name'] for m in data.get('models', [])]
            
            if model in available_models:
                return {
                    "status": "online",
                    "message": f"Ollama funcionando com {model}",
                    "host": host,
                    "model": model,
                    "available_models": available_models
                }
            else:
                return {
                    "status": "model_missing",
                    "message": f"Modelo {model} n√£o encontrado",
                    "host": host,
                    "model": model,
                    "available_models": available_models
                }
        else:
            return {
                "status": "error",
                "message": "Erro ao verificar modelos",
                "host": host,
                "model": model
            }
            
    except requests.exceptions.ConnectionError:
        return {
            "status": "offline",
            "message": "Ollama n√£o est√° rodando",
            "host": config.get("host", "localhost:11434"),
            "model": config.get("model", "deepseek-r1:7b")
        }
    except Exception as e:
        return {
            "status": "error",
            "message": f"Erro: {str(e)}",
            "host": config.get("host", "localhost:11434"),
            "model": config.get("model", "deepseek-r1:7b")
        }

'''
        
        # Inserir fun√ß√£o antes de show_dashboard
        insert_pos = content.find('def show_dashboard():')
        if insert_pos != -1:
            content = content[:insert_pos] + ollama_status_function + '\n' + content[insert_pos:]

# 3. Adicionar fun√ß√£o show_llm
show_llm_function = '''
def show_llm():
    """Exibe p√°gina de LLM e IA."""
    st.header("ü§ñ LLM & Intelig√™ncia Artificial")
    st.markdown("Configure e teste as funcionalidades de IA para an√°lise financeira inteligente.")
    
    tab1, tab2, tab3, tab4 = st.tabs(["üîß Status", "üè∑Ô∏è Categoriza√ß√£o", "üí° Insights", "üîÑ Recorrentes"])
    
    with tab1:
        st.subheader("Status do Ollama")
        
        # Verificar status do Ollama
        ollama_info = get_ollama_status()
        
        if ollama_info["status"] == "online":
            st.success(f"‚úÖ {ollama_info['message']}")
            
            col1, col2 = st.columns(2)
            with col1:
                st.info(f"**Host:** {ollama_info['host']}")
                st.info(f"**Modelo Ativo:** {ollama_info['model']}")
            
            with col2:
                if "available_models" in ollama_info:
                    st.info(f"**Modelos Dispon√≠veis:** {len(ollama_info['available_models'])}")
                    with st.expander("Ver todos os modelos"):
                        for model in ollama_info['available_models']:
                            st.write(f"‚Ä¢ {model}")
            
            # Teste r√°pido de categoriza√ß√£o
            st.markdown("#### üß™ Teste de Categoriza√ß√£o")
            test_transaction = st.text_input(
                "Digite uma transa√ß√£o para testar:",
                value="PIX Supermercado ABC 150.00",
                help="Exemplo: PIX Supermercado ABC 150.00"
            )
            
            if st.button("üîç Categorizar"):
                with st.spinner("Categorizando..."):
                    try:
                        import requests
                        test_data = {
                            "model": ollama_info["model"],
                            "prompt": f"Categorize esta transa√ß√£o financeira em uma das categorias: Alimenta√ß√£o, Transporte, Moradia, Sa√∫de, Educa√ß√£o, Lazer, Renda, Outros. Responda apenas a categoria: {test_transaction}",
                            "stream": False,
                            "options": {"num_predict": 10, "temperature": 0.1}
                        }
                        response = requests.post(f"{ollama_info['host']}/api/generate", json=test_data, timeout=20)
                        if response.status_code == 200:
                            result = response.json()
                            category = result.get("response", "").strip()
                            st.success(f"‚úÖ **Categoria sugerida:** {category}")
                        else:
                            st.error("‚ùå Erro na categoriza√ß√£o")
                    except Exception as e:
                        st.error(f"‚ùå Erro: {str(e)}")
                        
        elif ollama_info["status"] == "offline":
            st.error(f"‚ùå {ollama_info['message']}")
            st.info("üí° **Como resolver:**")
            st.code("ollama serve")
            st.markdown("Execute o comando acima no terminal para iniciar o Ollama.")
            
        elif ollama_info["status"] == "model_missing":
            st.warning(f"‚ö†Ô∏è {ollama_info['message']}")
            st.info("üí° **Como resolver:**")
            st.code(f"ollama pull {ollama_info['model']}")
            st.markdown("Execute o comando acima para baixar o modelo.")
            
            if "available_models" in ollama_info and ollama_info["available_models"]:
                st.info("**Modelos dispon√≠veis:**")
                for model in ollama_info["available_models"]:
                    st.write(f"‚Ä¢ {model}")
                    
        else:
            st.error(f"‚ùå {ollama_info['message']}")
        
        # Link para configura√ß√µes
        st.markdown("---")
        st.info("üîß **Configurar Ollama:** V√° para Configura√ß√µes ‚Üí Sistema ‚Üí Configura√ß√£o do Ollama")
    
    with tab2:
        st.subheader("Categoriza√ß√£o Autom√°tica")
        st.info("üöß Em desenvolvimento: Interface para configurar regras de categoriza√ß√£o autom√°tica")
        
    with tab3:
        st.subheader("Insights Inteligentes")
        st.info("üöß Em desenvolvimento: An√°lise inteligente de padr√µes de gastos")
        
    with tab4:
        st.subheader("Detec√ß√£o de Recorrentes")
        st.info("üöß Em desenvolvimento: Identifica√ß√£o autom√°tica de gastos recorrentes")

'''

# Inserir fun√ß√£o show_llm antes da fun√ß√£o main
insert_pos = content.find('def main():')
if insert_pos != -1:
    content = content[:insert_pos] + show_llm_function + '\n' + content[insert_pos:]

# 4. Adicionar roteamento para p√°gina LLM
old_routing = '''    elif page == "‚öôÔ∏è Configura√ß√µes":
        show_settings()'''

new_routing = '''    elif page == "ü§ñ LLM":
        show_llm()
    elif page == "‚öôÔ∏è Configura√ß√µes":
        show_settings()'''

content = content.replace(old_routing, new_routing)

# 5. Corrigir status do Ollama no Dashboard
if "ollama_status = \"unknown\"" in content:
    old_ollama_dashboard = '''            ollama_status = "unknown"
            status_color = "üî¥"
            st.write(f"{status_color} **Ollama:** {ollama_status}")'''
    
    new_ollama_dashboard = '''            ollama_info = get_ollama_status()
            if ollama_info["status"] == "online":
                status_color = "üü¢"
                ollama_status = "funcionando"
            elif ollama_info["status"] == "offline":
                status_color = "üî¥"
                ollama_status = "offline"
            elif ollama_info["status"] == "model_missing":
                status_color = "üü°"
                ollama_status = "modelo n√£o encontrado"
            else:
                status_color = "üî¥"
                ollama_status = "erro"
            st.write(f"{status_color} **Ollama:** {ollama_status}")'''
    
    content = content.replace(old_ollama_dashboard, new_ollama_dashboard)

# Salvar arquivo
with open('streamlit_app.py', 'w') as f:
    f.write(content)

print("‚úÖ P√°gina LLM adicionada e status corrigido")
EOF

# Verificar se as mudan√ßas foram aplicadas
if grep -q "ü§ñ LLM" streamlit_app.py; then
    log "‚úÖ P√°gina LLM adicionada ao menu"
else
    log "‚ö†Ô∏è Verificar se p√°gina LLM foi adicionada"
fi

if grep -q "def show_llm" streamlit_app.py; then
    log "‚úÖ Fun√ß√£o show_llm criada"
else
    log "‚ö†Ô∏è Verificar se fun√ß√£o show_llm foi criada"
fi

# Verificar sintaxe
python3 -m py_compile streamlit_app.py 2>/dev/null && log "‚úÖ Sintaxe Python v√°lida" || log "‚ö†Ô∏è Verificar sintaxe"

echo ""
echo "=================================================="
echo -e "${GREEN}‚úÖ P√°gina LLM adicionada com sucesso!${NC}"
echo ""
echo "Funcionalidades implementadas:"
echo "‚Ä¢ ü§ñ Nova p√°gina LLM no menu de navega√ß√£o"
echo "‚Ä¢ üîß Status detalhado do Ollama"
echo "‚Ä¢ üß™ Teste de categoriza√ß√£o integrado"
echo "‚Ä¢ üìä Dashboard com status real do Ollama"
echo "‚Ä¢ üí° Dicas de resolu√ß√£o de problemas"
echo "‚Ä¢ üîó Links para configura√ß√£o"
echo ""
echo "Agora reinicie o Streamlit:"
echo "‚Ä¢ Ctrl+C para parar"
echo "‚Ä¢ ./start_simple.sh para reiniciar"
echo ""
echo "Voc√™ ver√° a nova p√°gina ü§ñ LLM no menu!"
echo "=================================================="

